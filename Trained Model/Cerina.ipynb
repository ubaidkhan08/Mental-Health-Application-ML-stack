{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484d9218",
   "metadata": {},
   "source": [
    "# Project Cerina!\n",
    "The first approach for a machine learning model fine-tuned to detect when a person is showing self-harming tendencies by analyzing their texts. The second approach would follow advance fine-tuning using OpenAI api models (check the other folder for the detailed walkthrough). Hope you like it! We have used the model obtained below into developing a small application:\n",
    "\n",
    "\n",
    "`Web App Link:`   \n",
    "https://ubaidkhan08-mental-health-application-ml-st-appstreamlit-efzurp.streamlit.app/\n",
    "\n",
    "`GitHub Repo:`    \n",
    "https://github.com/ubaidkhan08/Mental-Health-Application-ML-stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255dd1c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e993b",
   "metadata": {},
   "source": [
    "# Reading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7740c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62f06c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc2248",
   "metadata": {},
   "source": [
    "# Reading the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87b3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a4d1146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3          i need helpjust help me im crying so hard      suicide\n",
       "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a8bf6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3715d1d",
   "metadata": {},
   "source": [
    "# Preprocessing!\n",
    "Here, we encode the labels using LabelEncoder, split the data into training and test sets, and tokenize the text with a maximum vocabulary size of 5000. This approach enables the machine learning model to learn from the data by converting the text into numerical inputs that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d865431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "X = df['text'].values\n",
    "y = df['class'].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4357b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de77379",
   "metadata": {},
   "source": [
    "# Loading the saved tokenizer file (for later usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2595585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "    tokenizer_json = f.read()\n",
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00668b38",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304982a4",
   "metadata": {},
   "source": [
    "# Model training!\n",
    "Now, we create a Sequential model using Keras with an embedding layer to learn a dense vector representation of the text, an LSTM layer for processing sequential data, and a dense output layer with a `sigmoid activation` function for binary classification. The model is trained using the binary_crossentropy loss function and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6684be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5802/5802 [==============================] - 358s 61ms/step - loss: 0.3464 - accuracy: 0.8559 - val_loss: 0.2138 - val_accuracy: 0.9199\n",
      "Epoch 2/10\n",
      "5802/5802 [==============================] - 359s 62ms/step - loss: 0.1891 - accuracy: 0.9293 - val_loss: 0.1968 - val_accuracy: 0.9239\n",
      "Epoch 3/10\n",
      "5802/5802 [==============================] - 387s 67ms/step - loss: 0.1726 - accuracy: 0.9364 - val_loss: 0.1805 - val_accuracy: 0.9327\n",
      "Epoch 4/10\n",
      "5802/5802 [==============================] - 401s 69ms/step - loss: 0.1623 - accuracy: 0.9396 - val_loss: 0.1747 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "5802/5802 [==============================] - 404s 70ms/step - loss: 0.1546 - accuracy: 0.9429 - val_loss: 0.1745 - val_accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "5802/5802 [==============================] - 407s 70ms/step - loss: 0.1485 - accuracy: 0.9452 - val_loss: 0.1761 - val_accuracy: 0.9346\n",
      "Epoch 7/10\n",
      "5802/5802 [==============================] - 411s 71ms/step - loss: 0.1431 - accuracy: 0.9471 - val_loss: 0.1802 - val_accuracy: 0.9323\n",
      "Epoch 8/10\n",
      "5802/5802 [==============================] - 424s 73ms/step - loss: 0.1387 - accuracy: 0.9487 - val_loss: 0.1829 - val_accuracy: 0.9304\n",
      "Epoch 9/10\n",
      "5802/5802 [==============================] - 429s 74ms/step - loss: 0.1341 - accuracy: 0.9505 - val_loss: 0.1770 - val_accuracy: 0.9348\n",
      "Epoch 10/10\n",
      "5802/5802 [==============================] - 453s 78ms/step - loss: 0.1293 - accuracy: 0.9519 - val_loss: 0.1787 - val_accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9aece05e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding='post', maxlen=100)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=100)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 16, input_length=100),\n",
    "    tf.keras.layers.LSTM(64, dropout=0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50d93f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80730678",
   "metadata": {},
   "source": [
    "# Saving & loading the model files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model.save('my_model.h5')\n",
    "loaded_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5eab41",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42103ac",
   "metadata": {},
   "source": [
    "# Model Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00bb2318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       483\n",
      "           1       0.95      0.98      0.96       517\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = []\n",
    "for x in X_train[0:1000]:\n",
    "    a = health(x)\n",
    "    y_pred.append(a)\n",
    "\n",
    "new = np.array(y_pred)\n",
    "new = encoder.fit_transform(new)\n",
    "\n",
    "print(classification_report(y_train[0:1000], new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fd06d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cc98d",
   "metadata": {},
   "source": [
    "# Predictions using our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a082be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Self-harmful'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def health(text):\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    text_padded = pad_sequences(text_sequence, padding='post', maxlen=100)\n",
    "    prediction = loaded_model.predict(text_padded)\n",
    "\n",
    "    if prediction[0] >= 0.5:\n",
    "        return \"Self-harmful\"\n",
    "\n",
    "    elif prediction[0] < 0.5:\n",
    "        return \"Normal\"\n",
    "    \n",
    "    \n",
    "health(\"I'm feeling really down today. I don't know if I can take it anymore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f2863",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9be23b",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
